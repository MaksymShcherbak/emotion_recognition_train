


import os
import PIL
import numpy as np
import pandas as pd
import imblearn
import cv2
import keras
import tensorflow
import matplotlib.pyplot as plt
import seaborn as sns





from PIL import Image

def load_images_from_folder(folder):
    images = []
    labels = []
    for label in os.listdir(folder):
        label_folder = os.path.join(folder, label)
        if os.path.isdir(label_folder):
            for filename in os.listdir(label_folder):
                img_path = os.path.join(label_folder, filename)
                img = Image.open(img_path)
                img_array = np.array(img)
                images.append(img_array)
                labels.append(label)
    return images, labels

def create_dataframe(train_folder, test_folder):
    train_images, train_labels = load_images_from_folder(train_folder)
    test_images, test_labels = load_images_from_folder(test_folder)
    
    images = train_images + test_images
    labels = train_labels + test_labels
    
    df = pd.DataFrame({
        'pixels': images,
        'emotion': labels
    })
    return df

train_folder = './data/train'
test_folder = './data/test'

df = create_dataframe(train_folder, test_folder)
df.head()


label_names = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']
num_classes = len(label_names)


df['emotion'] = df['emotion'].str.lower()
df['emotion'] = df['emotion'].map({label_names[i]: i for i in range(num_classes)})


data = df
data.info()








data['emotion'].value_counts()


from imblearn.over_sampling import RandomOverSampler

X = data.drop('emotion', axis=1)
y = data['emotion']

oversampler = RandomOverSampler(random_state=0)

X_resampled, y_resampled = oversampler.fit_resample(X, y)
data = pd.concat([pd.DataFrame(X_resampled), pd.DataFrame(y_resampled, columns=['emotion'])], axis=1)


data['emotion'].value_counts()





image_shape = (48,48)
data['pixels'] = data['pixels'].apply(lambda x: x / 255)


np.random.seed(100)

n_random_indices = 5
random_indices = np.random.choice(len(data), n_random_indices, replace=False)

fig, axes = plt.subplots(1, n_random_indices, figsize=(10, 5))

for i, idx in enumerate(random_indices):
    axes[i].imshow(data['pixels'][idx], cmap='gray')
    axes[i].axis('off')
    axes[i].set_title(f'#{idx+1} {label_names[data["emotion"][idx]]}')

plt.show()





from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split

ohe = OneHotEncoder()
X, y = np.stack(data['pixels']), ohe.fit_transform(data['emotion'].values.reshape(-1, 1)).toarray()

X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val)


print(f"Training data length: X {X_train.shape}, Y {y_train.shape}")
print(f"Validation data length: X {X_val.shape},Y {y_val.shape}")
print(f"Testing data length: X {X_test.shape}, Y {y_test.shape}")


np.unique(np.argmax(y_test, axis=1), return_counts=True)








from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, BatchNormalization


keras.utils.set_random_seed(1)

model = Sequential([
    Flatten(input_shape=X_train[0].shape),
    Dense(256, activation='relu'),  
    BatchNormalization(),
    Dense(64, activation='relu'),
    BatchNormalization(),
    Dense(16, activation='relu'),
    Dense(num_classes, activation='softmax') 
])

model.compile(loss='categorical_crossentropy',
              optimizer=keras.optimizers.Adam(learning_rate=0.001),
              metrics=['accuracy'])


model.summary()


from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

history = model.fit(X_train, y_train, 
                    batch_size=64, 
                    epochs=20,
                    validation_data=(X_val, y_val),
                    callbacks=[
                        EarlyStopping(patience=5, restore_best_weights=True),
                        ReduceLROnPlateau(factor=0.1, patience=3, min_lr=1e-6)
                    ])


def plot_history(history):
    # Create subplots
    fig, axes = plt.subplots(1, 2, figsize=(10, 3))

    # Plot accuracy
    axes[0].plot(history.history['accuracy'], label='Train Accuracy')
    axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy')
    axes[0].set_title('Model Accuracy')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Accuracy')
    axes[0].legend()

    # Plot loss
    axes[1].plot(history.history['loss'], label='Train Loss')
    axes[1].plot(history.history['val_loss'], label='Validation Loss')
    axes[1].set_title('Model Loss')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Loss')
    axes[1].legend()

    # Adjust layout
    plt.tight_layout()
    plt.show()

plot_history(history)


score = model.evaluate(X_val, y_val)
print('val loss:', score[0])
print('val accuracy:', score[1])


private_test_score = model.evaluate(X_test, y_test)
print('test loss:', private_test_score[0])
print('test accuracy:', private_test_score[1])


y_pred = model.predict(X_test)
predicted_labels = ohe.inverse_transform(y_pred).flatten()


from sklearn.metrics import precision_score, recall_score, f1_score, classification_report

def do_classification_report(y_test, y_pred):
    y_test_inv = np.argmax(y_test, axis=1)
    y_pred_inv = np.argmax(y_pred, axis=1)
    
    precision = precision_score(y_test_inv, y_pred_inv, average='macro')
    recall = recall_score(y_test_inv, y_pred_inv, average='macro')
    f1 = f1_score(y_test_inv, y_pred_inv, average='macro')
    print(f'Precision: {precision:.4f}')
    print(f'Recall: {recall:.4f}')
    print(f'F1-score: {f1:.4f}')
    
    class_report = classification_report(y_test_inv, y_pred_inv)
    print('\nClassification Report:\n', class_report)

do_classification_report(y_test, y_pred)


from sklearn.metrics import confusion_matrix

def plot_confusion_matrix(y_true, y_pred, class_names):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(4, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.show()

plot_confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1), label_names)


def plot_random_images_with_histogram(X_test, y_test, y_pred, label_names, num_classes, image_shape, show_errors=False):
    np.random.seed(100)
    
    y_test_inv = np.argmax(y_test, axis=1)
    y_pred_inv = np.argmax(y_pred, axis=1)

    if show_errors:
        incorrect_indices = np.where(y_test_inv != y_pred_inv)[0]
        random_indices = np.random.choice(incorrect_indices, size=3, replace=False)
    else:
        correct_indices = np.where(y_test_inv == y_pred_inv)[0]
        random_indices = np.random.choice(correct_indices, size=3, replace=False)

    fig, axes = plt.subplots(3, 2, figsize=(12, 6))

    for i, idx in enumerate(random_indices):
        axes[i, 0].imshow(X_test[idx], cmap='gray')
        axes[i, 0].set_title(f'True Label: {label_names[y_test_inv[idx]]}')
        axes[i, 0].axis('off')

        bar_colors = ['skyblue'] * num_classes
        bar_colors[y_pred_inv[idx]] = 'salmon'
        axes[i, 1].bar(np.arange(num_classes), y_pred[idx], color=bar_colors)
        axes[i, 1].set_title(f'Predicted Label: {label_names[y_pred_inv[idx]]}')
        axes[i, 1].set_xlabel('Class')
        axes[i, 1].set_ylabel('Probability')
        axes[i, 1].set_xticks(np.arange(num_classes))
        axes[i, 1].set_xticklabels(label_names, rotation=0)
        axes[i, 1].set_ylim([0, 1]) 

    plt.tight_layout()
    plt.show()

def plot_correct_image_labels(X_test, y_test, y_pred, label_names, num_classes, image_shape):
    plot_random_images_with_histogram(X_test, y_test, y_pred, label_names, num_classes, image_shape)

def plot_wrong_image_labels(X_test, y_test, y_pred, label_names, num_classes, image_shape):
    plot_random_images_with_histogram(X_test, y_test, y_pred, label_names, num_classes, image_shape, True)


plot_correct_image_labels(X_test, y_test, y_pred, label_names, num_classes, image_shape)


plot_random_images_with_histogram(X_test, y_test, y_pred, label_names, num_classes, image_shape, True)


model.save('model_dense.keras')





from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Reshape


keras.utils.set_random_seed(1)

model = Sequential([
    Reshape(image_shape + (1,), input_shape=image_shape),
    Conv2D(32, (5, 5), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(32, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(32, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(64, activation='relu',),
    BatchNormalization(),
    Dense(32, activation='relu',),   
    Dense(num_classes, activation='softmax')
])

model.compile(loss='categorical_crossentropy',
              optimizer=keras.optimizers.Adam(learning_rate=0.001),
              metrics=['accuracy'])



model.summary()


history = model.fit(X_train, y_train, 
                    batch_size=64, 
                    epochs=30,
                    validation_data=(X_val, y_val),
                    callbacks=[
                        EarlyStopping(patience=5, restore_best_weights=True),
                        ReduceLROnPlateau(factor=0.1, patience=3, min_lr=1e-6)
                    ])


plot_history(history)


score = model.evaluate(X_val, y_val)
print('val loss:', score[0])
print('val accuracy:', score[1])


score = model.evaluate(X_test, y_test)
print('test loss:', score[0])
print('test accuracy:', score[1])


y_pred = model.predict(X_test)
predicted_labels = ohe.inverse_transform(y_pred).flatten()


do_classification_report(y_test, y_pred)


plot_confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1), label_names)


plot_correct_image_labels(X_test, y_test, y_pred, label_names, num_classes, image_shape)


plot_wrong_image_labels(X_test, y_test, y_pred, label_names, num_classes, image_shape)


model.save('model_convolutional.keras')





from keras.applications import MobileNetV2
from keras.layers import GlobalAveragePooling2D, Input, Reshape
from keras.preprocessing.image import ImageDataGenerator


keras.utils.set_random_seed(1)

base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

for layer in base_model.layers:
    layer.trainable = False

model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(num_classes, activation='softmax')
])

model.compile(loss='categorical_crossentropy',
              optimizer=keras.optimizers.Adam(learning_rate=0.001),
              metrics=['accuracy'])





def reshape_and_resize_generator(X, new_size=(224, 224)):
    for i in range(X.shape[0]):
        image = np.expand_dims(X[i], axis=-1)
        image = np.repeat(image, 3, axis=-1)
        resized_image = cv2.resize(image, new_size)
        yield resized_image

def batch_generator(X, y, batch_size, new_size=(224, 224)):
    while True:
        for start in range(0, X.shape[0], batch_size):
            end = min(start + batch_size, X.shape[0])
            batch_X = X[start:end]
            batch_y = y[start:end]
            resized_batch_X = np.array([img for img in reshape_and_resize_generator(batch_X, new_size)])
            yield resized_batch_X, batch_y

batch_size = 128
train_generator = batch_generator(X_train, y_train, batch_size)
val_generator = batch_generator(X_val, y_val, batch_size)
test_generator = batch_generator(X_test, y_test, batch_size)


history = model.fit(train_generator,
                    steps_per_epoch=len(X_train) // batch_size,
                    validation_data=val_generator,
                    validation_steps=len(X_val) // batch_size,
                    epochs=30,
                    callbacks=[
                        EarlyStopping(patience=5, restore_best_weights=True),
                        ReduceLROnPlateau(factor=0.1, patience=3, min_lr=1e-6)
                    ])


plot_history(history)


score = model.evaluate(val_generator, steps=len(X_val) // batch_size)
print('val loss:', score[0])
print('val accuracy:', score[1])


private_test_score = model.evaluate(test_generator, steps=len(X_test) // batch_size)
print('test loss:', private_test_score[0])
print('test accuracy:', private_test_score[1])


y_pred = model.predict(test_generator, steps = len(X_test) // batch_size)
if len(X_train) % batch_size != 0:
    remaining_samples = len(X_test) % batch_size
    last_batch_X = X_test[-remaining_samples:]
    last_batch_y_pred = model.predict(np.array([img for img in reshape_and_resize_generator(last_batch_X)]))
    y_pred = np.concatenate((y_pred, last_batch_y_pred), axis=0)

predicted_labels = ohe.inverse_transform(y_pred).flatten()


do_classification_report(y_test, y_pred)


plot_confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1), label_names)


plot_correct_image_labels(X_test, y_test, y_pred, label_names, num_classes, image_shape)


plot_wrong_image_labels(X_test, y_test, y_pred, label_names, num_classes, image_shape)


model.save('model_mobilenetv2.keras')



